<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Applied Statistics | ğšƒğš›ğšŠğš—ğšœğš™ğš˜ğš—ğšœğšğšğš›</title>
    <link>/categories/applied-statistics/</link>
      <atom:link href="/categories/applied-statistics/index.xml" rel="self" type="application/rss+xml" />
    <description>Applied Statistics</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2018 Kapil Khanal</copyright><lastBuildDate>Sun, 19 Jan 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/aph-salt-spring-zoom.jpg</url>
      <title>Applied Statistics</title>
      <link>/categories/applied-statistics/</link>
    </image>
    
    <item>
      <title>Minnesota Lake Project</title>
      <link>/post/minnesota-lake-project/</link>
      <pubDate>Sun, 19 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/post/minnesota-lake-project/</guid>
      <description>

&lt;h2 id=&#34;so-you-have-a-good-model-want-to-actually-use-it&#34;&gt;So you have a good model? Want to actually use it?&lt;/h2&gt;

&lt;h5 id=&#34;prototype-grade-model-workflow-to-production-land-workflow&#34;&gt;Prototype grade model workflow to Production land workflow&lt;/h5&gt;

&lt;p&gt;Making a good model is awesome. It does takes enormous amount of experimentation and research but once we have model, that is an eureka moment. But to actually use the model in production is a whole another pain. Recently I have been learning ways to deploy models.&lt;br /&gt;
&lt;img src=&#34;/post/2020-01-19-minnesota-lake-project_files/mlsystem.jpg&#34; alt=&#34;Source:Manning Reactive Machine learning book&#34; /&gt;
Source: Reactive machine learning book&lt;/p&gt;

&lt;h4 id=&#34;model-predictions-as-webservice&#34;&gt;Model Predictions as WebService&lt;/h4&gt;

&lt;p&gt;Now,as we can see it is a lifecycle. There is a lot of nuances on deploying models. The workflow has to be reproducible,elastic and easy to manage. If you end up changing the model, the infrastructure should not have to be changed. For example, I used a simple regression model for this project, now if i am training random forest model, the parts that needs to be changed should be easily changed that is i collect all the parameters and file locations, data locations on on file say &lt;strong&gt;&lt;em&gt;config&lt;/em&gt;&lt;/strong&gt; file then that will not be changed.Similarly, if i separate the feature engineering, feature selection part , data validation etc on their own separate files then it will be easy to deploy. I can always train two different model and put it in the python package or cloud location like Pypi,S3 etc then i can easily retrieve those models and use it in the flask API i design just to serve the model.&lt;/p&gt;

&lt;p&gt;Thinking each service as a different code repoisitory. We will have three different repos.
&lt;li&gt;Python package for retrieving data, training model and uploading final model to PyPi,or S3&lt;/li&gt;
&lt;li&gt;ML api: Flask Application to serve the model by downloading the model from PyPi/S3 and exposing a API to get the data and return the prediction&lt;/li&gt;
&lt;li&gt;Another Flask/Front-end framework to get the json from API and populate the dashboard with plots and predictions&lt;/li&gt;&lt;/p&gt;

&lt;p&gt;While learning about this, I came across the idea of DataOps and MLOps. I think in future, most softwares will be ML softwares doing real time prediction and inference with very little slowdown. Wait, don&amp;rsquo;t human do that?&lt;/p&gt;

&lt;p&gt;All these different services are extensive on their own. Without a dedicated team, these services will not succeed. But this exercise was to get a general understanding of the overall ecosystem of Data and ML system. To be a good data scientist, i think it is good to get a lay of the land.&lt;/p&gt;

&lt;p&gt;If you like to see the end product, this link will take you there &lt;a href = &#34;http://lakedashboard.team/&#34;&gt;lake Dashboard&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fake Data, Valid Models , Better Understanding</title>
      <link>/post/2019-09-10-cdvdv/</link>
      <pubDate>Sun, 21 Aug 2016 21:13:14 -0500</pubDate>
      <guid>/post/2019-09-10-cdvdv/</guid>
      <description>


&lt;div id=&#34;can-fake-data-help-understand-the-inference-process-and-model-usage&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Can fake data help understand the inference process and model usage?&lt;/h2&gt;
&lt;p&gt;Data science and statistics courses deal with either already famous datasets like iris dataset or Iowa home saleprice. These datasets have their own place. They are generated in real life. After all, statistics is about dealing with real life in a more formal way. But, i donâ€™t think these datasets necessary prepare students on working with the real life data. This case is similar to Applied Mathematics modeling.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;No one throws the differential equations that occur in nature right away at students when beginning to learn about differential equations&lt;/code&gt;
&lt;br&gt;
Almost all of mathematics concepts are playfully introduced with problems that we know answers of, that we understand all the nots and bolts.One can actually see the progression of the complexity of the problems and solve them one by one. When some problems seems to be unsolvable then there is a transition to a newer class of concepts.&lt;/p&gt;
&lt;p&gt;Real life data by definition we donâ€™t understand fully. One can interpret it on different ways. A lot of people argue that neural networks are awesome and can beat any other models. Maybe this is true because the universe of real life dataset a lot of people learn with are best modelled by neural networks. &lt;br&gt;
Instead, if people had learned to train different models on each fake datasets designed to learn the nuts and bolts of the models, they would have seen why sometimes a different model is worth more. &lt;br&gt;&lt;/p&gt;
&lt;p&gt;I think this way of looking at the dataset will give intution about the geometry of dataset and mechanics of the model.&lt;/p&gt;
&lt;p&gt;I found this awesome website where you can draw and download the dataset&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
